{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b08d6-477a-47ff-9f8f-f111a66d8efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "MBD: Multi b-value Denoising of Diffusion Magnetic Resonance Images\n",
    "Jakub Jurek (1), Andrzej Materka (1), Kamil Ludwisiak (2), Agata Majos (3), Filip Szczepankiewicz (4)\n",
    "\n",
    "(1) Institute of Electronics, Lodz University of Technology, Aleja Politechniki 10, PL-93590 Lodz, Poland\n",
    "(2) Department of Diagnostic Imaging, Independent Public Health Care, Central Clinical Hospital, Medical University of Lodz, Pomorska 2 51, PL-92213 Lodz, Poland\n",
    "(3) Department of Radiology, Medical University of Lodz, Lodz, Poland\n",
    "(4) Medical Radiation Physics, Lund University, Barngatan 4, 22185 Lund, Sweden\n",
    "\n",
    "This code demonstrates how to use MBD. A brain phantom dataset is used.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# set path to the working directory\n",
    "path = \"/home/likewise-open/ADM/jakub.jurek/MBD/case2/\"\n",
    "os.chdir(path)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mbd_tools import addRicianNoise, get_dimg_patches, getRicianM, SRData, DnCNN, DnCNNe, try_gpu, plot_contour\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "# theoretical intensity of pure white matter voxels for spin echo MRI and parameters:\n",
    "# proton density = 0.77, T1 = 500 ms, T2 = 70 ms, m0=1000, TR=6700, TE = 100\n",
    "wm = 0.77*1000*np.exp(-100/70)*(1-np.exp(-6700/500))\n",
    "\n",
    "data = []\n",
    "\n",
    "# IDs of lesion sets merged with the phantom\n",
    "ids = [1,2,3] #[1,2,3,4,5,6,7,8]\n",
    "\n",
    "for pat_id in ids:\n",
    "    data.append(nib.load(path+'data//dti_L%sl.nii.gz'%pat_id).get_fdata()[:,:,60:120,:])\n",
    "data = np.concatenate(data,-2)\n",
    "\n",
    "# mask for voxels where lesion content is > 0%\n",
    "masks = []\n",
    "for pat_id in range(len(ids)):\n",
    "    masks.append(nib.load(path+'data//dtiMask2_L1l.nii.gz').get_fdata()[:,:,60:120])\n",
    "masks = np.concatenate(masks,-1)\n",
    "\n",
    "# # mask for voxels where lesion content is 100%\n",
    "# masks2 = []\n",
    "# for pat_id in range(len(ids)):\n",
    "#     masks2.append(nib.load(path+'data//dtiMask_L1l.nii.gz').get_fdata()[:,:,60:120])\n",
    "# masks2 = np.concatenate(masks2,-1)\n",
    "\n",
    "# noise levels relative to WM intensity\n",
    "nlevels = np.array([0.01, 0.03, 0.05])*wm\n",
    "nlev = 1 # set noise level index relative to nlevels\n",
    "\n",
    "m  = addRicianNoise(data,nlevels[nlev]) # input images\n",
    "targetx  = addRicianNoise(data[:,:,:,2],nlevels[nlev]) # target images\n",
    "\n",
    "s = 72; d = -1\n",
    "vmx1 = np.mean(m[:,:,s,0])+2*np.std(m[:,:,s,0])\n",
    "vmx2 = np.mean(m[:,:,s,1])+2*np.std(m[:,:,s,1])\n",
    "vmx3 = np.mean(m[:,:,s,2])+2*np.std(m[:,:,s,2])\n",
    "\n",
    "fig, ax = plt.subplots(1,5,figsize=(10,5))\n",
    "ax[0].imshow(m[:,:,s,0],cmap='gray', vmin=0, vmax=vmx1)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].imshow(m[:,:,s,1],cmap='gray', vmin=0, vmax=vmx2)\n",
    "ax[1].set_axis_off()\n",
    "ax[2].imshow(m[:,:,s,2],cmap='gray', vmin=0, vmax=vmx3)\n",
    "ax[2].set_axis_off()\n",
    "ax[3].imshow(targetx[:,:,s],cmap='gray', vmin=0, vmax=vmx3)\n",
    "ax[3].set_axis_off()\n",
    "ax[4].imshow(data[:,:,s,-1],cmap='gray', vmin=0, vmax=vmx3)\n",
    "ax[4].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d19fdc-77f6-4bd7-83ca-6359277bcd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rilrx = m[:,:,:,m.shape[-1]-1] # take the DWI at b-val to denoise\n",
    "\n",
    "psize = 30 # training patch size\n",
    "stride = 30 # step between patch centers (for no gap/overlap set stride=psize )\n",
    "psizexv = stridexv =  rilrx.shape[0] # validation patch size (full slice)\n",
    "psizeyv = strideyv =  rilrx.shape[1]\n",
    "\n",
    "trains = list(range(targetx.shape[2])) # all slice indices\n",
    "\n",
    "sv = np.random.choice(np.arange(0,rilrx.shape[2]),20,replace=False) # 20 validation slices\n",
    "example = 5 # set validation example index to display during training\n",
    "\n",
    "[trains.remove(i) for i in sv] # training slice indices after removing validation slices\n",
    "\n",
    "# train patches\n",
    "p=[]\n",
    "p.append(get_dimg_patches(rilrx[:,:,trains,np.newaxis],psize,stride,psize,stride))\n",
    "x = np.stack(p,1)\n",
    "y = get_dimg_patches(targetx[:,:,trains,np.newaxis],psize,stride,psize,stride)\n",
    "\n",
    "# validation patches\n",
    "pv = []\n",
    "pv.append(get_dimg_patches(rilrx[:,:,sv,np.newaxis],psizexv,stridexv,psizeyv,strideyv))\n",
    "xv = np.stack(pv,1)\n",
    "yv = get_dimg_patches(targetx[:,:,sv,np.newaxis],psizexv,stridexv,psizeyv,strideyv)\n",
    "\n",
    "# adding extra b-values to the input arrays already containing b4000. Choose between indices to play. 1 is b1000, 0 is b0, 2 is b4000.\n",
    "for b in [1, 0]: \n",
    "\n",
    "    rilrx2 = addRicianNoise(np.mean(m[:,:,:,[b]],-1),nlevels[nlev])\n",
    "\n",
    "    p=[]\n",
    "    pv = []\n",
    "    p.append(get_dimg_patches(rilrx2[:,:,trains,np.newaxis],psize,stride,psize,stride))\n",
    "    pv.append(get_dimg_patches(rilrx2[:,:,sv,np.newaxis],psizexv,stridexv,psizeyv,strideyv))\n",
    "\n",
    "    x2 = np.stack(p,1)\n",
    "    xv2 = np.stack(pv,1)\n",
    "    \n",
    "    x = np.concatenate([x2,x],axis=1)\n",
    "    xv = np.concatenate([xv2,xv],axis=1)\n",
    "\n",
    "# # augmentation by rotation 90, 180, 270 degrees. Uncomment to turn on\n",
    "# x = np.concatenate([x,np.rot90(x,axes=(2,3)),np.rot90(x,2,axes=(2,3)),np.rot90(x,3,axes=(2,3))],0)\n",
    "# y = np.concatenate([y,np.rot90(y,axes=(1,2)),np.rot90(y,2,axes=(1,2)),np.rot90(y,3,axes=(1,2))],0)\n",
    "\n",
    "print(\"Data shapes\")\n",
    "print(\"Training inputs:\", x.shape, \"Training targets:\", y.shape, \"Validation inputs:\", xv.shape, \"Validation targets:\", yv.shape)\n",
    "\n",
    "pu = 'gpu' # set to 'cpu' if 'gpu' not accessible\n",
    "training_set = SRData([x,y], pu = pu)\n",
    "validation_set = SRData([xv,yv], pu = pu)\n",
    "\n",
    "mbdmodel = DnCNN(x.shape[1],5).to(try_gpu()) # create MBD model instance\n",
    "n2nmodel = DnCNN(1,5).to(try_gpu()) # create N2N model instance\n",
    "cnnemodel = DnCNNe(x.shape[1]-1,5).to(try_gpu()) # create CNNe model instance\n",
    "\n",
    "\"\"\"\n",
    "N2N and CNNe are used here for comparison only. Early stopping/best weights selection is done for MBD only. \n",
    "\"\"\"\n",
    "\n",
    "# Training hyperparameters\n",
    "loss_function = MSELoss()\n",
    "optimizer = Adam(mbdmodel.parameters(), lr=0.01)\n",
    "data_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "\n",
    "loss_function2 = MSELoss()\n",
    "optimizer2 = Adam(n2nmodel.parameters(), lr=0.01)\n",
    "\n",
    "loss_function3 = MSELoss()\n",
    "optimizer3 = Adam(cnnemodel.parameters(), lr=0.01)\n",
    "\n",
    "# Empirical value of minimal MSE loss in training, according to Lehtinen's Noise2Noise\n",
    "bottom = np.var(getRicianM(data[:,:,sv,-1],nlevels[nlev])-targetx[:,:,sv])\n",
    "\n",
    "# Empirical value of maximal MSE loss in training, according to Lehtinen's Noise2Noise\n",
    "top = np.var(rilrx[:,:,sv]-targetx[:,:,sv])\n",
    "\n",
    "max_epochs = 1000 # training will stop after max_epochs unless early stopping is executed\n",
    "\n",
    "clean = np.moveaxis(getRicianM(data[:,:,sv,-1],nlevels[nlev]),-1,0)\n",
    "\n",
    "tloss=np.zeros([max_epochs,1]) # training loss vector (MBD)\n",
    "vloss=np.zeros([max_epochs,1]) # validation loss vector\n",
    "tloss2=np.zeros([max_epochs,1]) # training loss vector (N2N)\n",
    "vloss2=np.zeros([max_epochs,1]) # validation loss vector\n",
    "tloss3=np.zeros([max_epochs,1]) # training loss vector (CNNe)\n",
    "vloss3=np.zeros([max_epochs,1]) # validation loss vector\n",
    "\n",
    "# Parameters of the custom early stopping module\n",
    "patience = 20\n",
    "min_delta = 10e-9\n",
    "epstep = 1\n",
    "model_history = {}\n",
    "firstc = 1000000000\n",
    "flag = 0\n",
    "\n",
    "for k in range(max_epochs):\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        train_input, train_target = batch\n",
    "        if k==0 and i==0:\n",
    "\n",
    "            val_input = validation_set.input\n",
    "            val_target = validation_set.target\n",
    "            \n",
    "            # display range for the validation images\n",
    "            maxv1 = np.mean(val_input[example,0,:,:].detach().cpu().numpy()) + 3*np.std(val_input[example,0,:,:].detach().cpu().numpy())\n",
    "            maxv2 = np.mean(val_input[example,1,:,:].detach().cpu().numpy()) + 3*np.std(val_input[example,1,:,:].detach().cpu().numpy())\n",
    "            maxv3 = np.mean(val_target[example,:,:].detach().cpu().numpy()) + 3*np.std(val_target[example,:,:].detach().cpu().numpy())\n",
    "            \n",
    "        train_output = mbdmodel(train_input)\n",
    "        loss_train = loss_function(train_output, train_target)\n",
    "        train_output2 = n2nmodel(train_input[:,-1:,:,:])\n",
    "        loss_train2 = loss_function(train_output2, train_target)\n",
    "        train_output3 = cnnemodel(train_input[:,:-1,:,:])\n",
    "        loss_train3 = loss_function(train_output3, train_target)\n",
    "        \n",
    "        # update models\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        loss_train2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        optimizer3.zero_grad()\n",
    "        loss_train3.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "    val_output = mbdmodel(val_input)\n",
    "    val_output2 = n2nmodel(val_input[:,-1:,:,:])\n",
    "    val_output3 = cnnemodel(val_input[:,:-1,:,:])\n",
    "\n",
    "    loss_val = loss_function(val_output[:,0,:,:], val_target)\n",
    "    loss_val2 = loss_function(val_output2[:,0,:,:], val_target)\n",
    "    loss_val3 = loss_function(val_output3[:,0,:,:], val_target)\n",
    "\n",
    "    tloss[k]=loss_train.item()\n",
    "    vloss[k]=loss_val.item()\n",
    "    tloss2[k]=loss_train2.item()\n",
    "    vloss2[k]=loss_val2.item()\n",
    "    tloss3[k]=loss_train3.item()\n",
    "    vloss3[k]=loss_val3.item()\n",
    "  \n",
    "    # Check validation cost every epstep epochs\n",
    "    the_loss=np.copy(vloss)\n",
    "    if (k) % epstep == 0:\n",
    "        min_cost=np.min(the_loss[:k+1])\n",
    "        if the_loss[k] == min_cost:\n",
    "            model_history['0']=mbdmodel\n",
    "            epoch_at_which = k\n",
    "        if k>0:\n",
    "            n=int(((k)/epstep))\n",
    "            delta=(the_loss[n-1]-the_loss[n])\n",
    "            if delta<min_delta or firstc<the_loss[n]:\n",
    "                if flag==0:\n",
    "                    firstc=the_loss[n-1]\n",
    "                flag+=1\n",
    "            elif firstc>=the_loss[n] and delta>=min_delta:\n",
    "                flag=0\n",
    "                model_history['prev']=mbdmodel\n",
    "            if flag>=patience:\n",
    "                break\n",
    "\n",
    "    if k % 10 == 0:\n",
    "\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.title(\"Learning curves\"); plt.xlabel('Epochs'); plt.ylabel('Validation MSE loss')\n",
    "        plt.subplot(121)\n",
    "        plt.plot(vloss[:k+1],'g',label='MBD')\n",
    "        plt.plot(vloss2[:k+1],'r',label='N2N')\n",
    "        plt.plot(vloss3[:k+1],'b',label='CNNe')\n",
    "        plt.plot(vloss[:k+1]*0 + top,'k-.',label='max error')\n",
    "        plt.plot(vloss[:k+1]*0 + bottom,'k--',label='min error')\n",
    "        plt.xlim([0,k+1])\n",
    "        plt.ylim([0.9*bottom, 1.1*top])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        diff = val_target[example,:,:].detach().cpu().numpy()-val_output[example,0,:,:].detach().cpu().numpy()\n",
    "        diff2 = val_target[example,:,:].detach().cpu().numpy()-val_output2[example,0,:,:].detach().cpu().numpy()\n",
    "        diff3 = val_target[example,:,:].detach().cpu().numpy()-val_output3[example,0,:,:].detach().cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.suptitle(\"Learning progress\")\n",
    "        plt.subplot(441); plt.title(\"Input, b4000\")\n",
    "        plt.imshow(val_input[example,-1,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3, cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(442); plt.title(\"Input, b1000\")\n",
    "        plt.imshow(val_input[example,1,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv2, cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(443); plt.title(\"Input, b0\")\n",
    "        plt.imshow(val_input[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv1, cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(444); plt.title(\"Target, b4000\")\n",
    "        plt.imshow(val_target[example,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(445); plt.title(\"MBD Output, b4000\")\n",
    "        plt.imshow(val_output[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(446); plt.title(\"N2N Output, b4000\")\n",
    "        plt.imshow(val_output2[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(447); plt.title(\"CNNe Output, b4000\")\n",
    "        plt.imshow(val_output3[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(448); plt.title(\"Clean Rician Biased, b4000\")\n",
    "        plt.imshow(clean[example,:,:],vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none'); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(449); plt.title(\"MBD residuals, MAE=%.2f\"%np.mean(np.abs(diff)))\n",
    "        plt.imshow(diff,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(4,4,10); plt.title(\"N2N residuals, MAE=%.2f\"%np.mean(np.abs(diff2)))\n",
    "        plt.imshow(diff2,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(4,4,11); plt.title(\"CNNe residuals, MAE=%.2f\"%np.mean(np.abs(diff3)))\n",
    "        plt.imshow(diff3,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(4,4,13); plt.title(\"MBD error\")\n",
    "        plt.imshow(val_output[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(4,4,14); plt.title(\"N2N error\")\n",
    "        plt.imshow(val_output2[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        plt.subplot(4,4,15); plt.title(\"CNNe error\")\n",
    "        plt.imshow(val_output3[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5); plt.axis('off'); plt.colorbar()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Smallest val. loss so far:\", min(vloss[:k+1]))\n",
    "\n",
    "model_mbd = model_history['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79655611-e048-42fd-856a-943a4e381555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show final result with lesion area contours\n",
    "\n",
    "fig, ax = plt.subplots(4,4,figsize=(15,15))\n",
    "\n",
    "ax[0][0].set_title(\"Input, b4000\")\n",
    "c1 = ax[0][0].imshow(val_input[example,-1,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3, cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[0][1].set_title(\"Input, b1000\")\n",
    "c2 = ax[0][1].imshow(val_input[example,1,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv2, cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[0][2].set_title(\"Input, b0\")\n",
    "c3 = ax[0][2].imshow(val_input[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv1, cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[0][3].set_title(\"Target, b4000\")\n",
    "c4 = ax[0][3].imshow(val_target[example,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[1][0].set_title(\"MBD Output, b4000\")\n",
    "c5 = ax[1][0].imshow(val_output[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[1][1].set_title(\"N2N Output, b4000\")\n",
    "c6 = ax[1][1].imshow(val_output2[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[1][2].set_title(\"CNNe Output, b4000\")\n",
    "c7 = ax[1][2].imshow(val_output3[example,0,:,:].detach().cpu().numpy(),vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[1][3].set_title(\"Clean Rician Biased, b4000\")\n",
    "c8 = ax[1][3].imshow(clean[example,:,:],vmin=0,vmax=maxv3,cmap=\"Greys_r\", interpolation = 'none')\n",
    "ax[2][0].set_title(\"MBD residuals, MAE=%.2f\"%np.mean(np.abs(diff)))\n",
    "c9 = ax[2][0].imshow(diff,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "ax[2][1].set_title(\"N2N residuals, MAE=%.2f\"%np.mean(np.abs(diff2)))\n",
    "c10 = ax[2][1].imshow(diff2,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "ax[2][2].set_title(\"CNNe residuals, MAE=%.2f\"%np.mean(np.abs(diff3)))\n",
    "c11 = ax[2][2].imshow(diff3,cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "ax[3][0].set_title(\"MBD error\")\n",
    "c12 = ax[3][0].imshow(val_output[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "ax[3][1].set_title(\"N2N error\")\n",
    "c13 = ax[3][1].imshow(val_output2[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "ax[3][2].set_title(\"CNNe error\")\n",
    "c14 = ax[3][2].imshow(val_output3[example,0,:,:].detach().cpu().numpy()-clean[example,:,:],cmap=\"Greys_r\", interpolation = 'none',vmin=-maxv3/5, vmax=maxv3/5)\n",
    "\n",
    "for a in ax:\n",
    "    for axx in a:\n",
    "        axx.set_axis_off()\n",
    "\n",
    "fig.colorbar(c1, ax=ax[0][0], label='', fraction=0.054)\n",
    "fig.colorbar(c2, ax=ax[0][1], label='', fraction=0.054)\n",
    "fig.colorbar(c3, ax=ax[0][2], label='', fraction=0.054)\n",
    "fig.colorbar(c4, ax=ax[0][3], label='', fraction=0.054)\n",
    "fig.colorbar(c5, ax=ax[1][0], label='', fraction=0.054)\n",
    "fig.colorbar(c6, ax=ax[1][1], label='', fraction=0.054)\n",
    "fig.colorbar(c7, ax=ax[1][2], label='', fraction=0.054)\n",
    "fig.colorbar(c8, ax=ax[1][3], label='', fraction=0.054)\n",
    "fig.colorbar(c9, ax=ax[2][0], label='', fraction=0.054)\n",
    "fig.colorbar(c10, ax=ax[2][1], label='', fraction=0.054)\n",
    "fig.colorbar(c11, ax=ax[2][2], label='', fraction=0.054)\n",
    "fig.colorbar(c12, ax=ax[3][0], label='', fraction=0.054)\n",
    "fig.colorbar(c13, ax=ax[3][1], label='', fraction=0.054)\n",
    "fig.colorbar(c14, ax=ax[3][2], label='', fraction=0.054)\n",
    "\n",
    "plot_contour(ax[0][0],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[0][1],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[0][2],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[0][3],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[1][0],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[1][1],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[1][2],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[1][3],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[2][0],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[2][1],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[2][2],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[3][0],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[3][1],masks[:,:,sv[example]]>2)\n",
    "plot_contour(ax[3][2],masks[:,:,sv[example]]>2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjcustom",
   "language": "python",
   "name": "jjcustom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
